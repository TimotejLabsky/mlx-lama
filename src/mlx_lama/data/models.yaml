# Default model registry for mlx-lama
# Users can override in ~/.mlx-lama/registry.yaml

models:
  # Qwen Coder - Best open-source coding models
  qwen-coder:32b:
    repo: mlx-community/Qwen2.5-Coder-32B-Instruct-4bit
    default_backend: mlx-lm
    quantization: 4bit
    description: Qwen 2.5 Coder 32B - Best open-source coding model

  qwen-coder:14b:
    repo: mlx-community/Qwen2.5-Coder-14B-Instruct-4bit
    default_backend: mlx-lm
    quantization: 4bit
    description: Qwen 2.5 Coder 14B - Fast coding model

  qwen-coder:7b:
    repo: mlx-community/Qwen2.5-Coder-7B-Instruct-4bit
    default_backend: mlx-lm
    quantization: 4bit
    description: Qwen 2.5 Coder 7B - Lightweight coding model

  # Qwen General - Excellent all-around models
  qwen:72b:
    repo: mlx-community/Qwen2.5-72B-Instruct-4bit
    default_backend: vllm
    quantization: 4bit
    description: Qwen 2.5 72B - Large reasoning model

  qwen:32b:
    repo: mlx-community/Qwen2.5-32B-Instruct-4bit
    default_backend: mlx-lm
    quantization: 4bit
    description: Qwen 2.5 32B - Excellent all-around model

  qwen:7b:
    repo: mlx-community/Qwen2.5-7B-Instruct-4bit
    default_backend: mlx-lm
    quantization: 4bit
    description: Qwen 2.5 7B - Fast general model

  # Llama - Meta's flagship models
  llama:70b:
    repo: mlx-community/Llama-3.3-70B-Instruct-4bit
    default_backend: vllm
    quantization: 4bit
    description: Llama 3.3 70B - GPT-4 class model

  llama:8b:
    repo: mlx-community/Llama-3.1-8B-Instruct-4bit
    default_backend: mlx-lm
    quantization: 4bit
    description: Llama 3.1 8B - Fast general model

  llama:3b:
    repo: mlx-community/Llama-3.2-3B-Instruct-4bit
    default_backend: mlx-lm
    quantization: 4bit
    description: Llama 3.2 3B - Very fast, lightweight

  # DeepSeek - Strong reasoning models
  deepseek-r1:32b:
    repo: mlx-community/DeepSeek-R1-Distill-Qwen-32B-4bit
    default_backend: mlx-lm
    quantization: 4bit
    description: DeepSeek R1 32B - Best reasoning model

  deepseek-coder:16b:
    repo: mlx-community/DeepSeek-Coder-V2-Lite-Instruct-4bit
    default_backend: mlx-lm
    quantization: 4bit
    description: DeepSeek Coder V2 16B - Fast coding

  # Mistral - Efficient European models
  mistral:7b:
    repo: mlx-community/Mistral-7B-Instruct-v0.3-4bit
    default_backend: mlx-lm
    quantization: 4bit
    description: Mistral 7B - Efficient general model

  codestral:22b:
    repo: mlx-community/Codestral-22B-v0.1-4bit
    default_backend: mlx-lm
    quantization: 4bit
    description: Codestral 22B - Mistral's coding model

  # Gemma - Google's efficient models
  gemma:9b:
    repo: mlx-community/gemma-2-9b-it-4bit
    default_backend: mlx-lm
    quantization: 4bit
    description: Gemma 2 9B - Google's efficient model

  gemma:2b:
    repo: mlx-community/gemma-2-2b-it-4bit
    default_backend: mlx-lm
    quantization: 4bit
    description: Gemma 2 2B - Very lightweight

  # Vision models
  llava:34b:
    repo: mlx-community/llava-v1.6-34b-4bit
    default_backend: mlx-lm
    quantization: 4bit
    description: LLaVA 34B - Vision + Language model

  qwen-vl:7b:
    repo: mlx-community/Qwen2-VL-7B-Instruct-4bit
    default_backend: mlx-lm
    quantization: 4bit
    description: Qwen2 VL 7B - Vision + Language
